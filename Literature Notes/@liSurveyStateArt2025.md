**Title**:: A Survey of State of the Art Large Vision Language Models: Alignment, Benchmark, Evaluations and Challenges
**Authors**:: Zongxia Li, Xiyang Wu, Hongyang Du, Fuxiao Liu, Huy Nghiem, Guangyao Shi
**Journal/Conference**:: 
**Citations**:
**DOI**:: 10.48550/arXiv.2501.02189
**Published**:: 2025
**Publiser**:: 

**Tags**::
**Links**:: http://arxiv.org/abs/2501.02189
**Created**:: <% tp.file.creation_date("YYYY-MM-DD") %>

# Abstract

Multimodal Vision Language Models (VLMs) have emerged as a transformative topic at the intersection of computer vision and natural language processing, enabling machines to perceive and reason about the world through both visual and textual modalities. For example, models such as CLIP [194], Claude [11], and GPT-4V [246] demonstrate strong reasoning and understanding abilities on visual and textual data and beat classical single modality vision models on zero-shot classification [94]. With their rapid advancements in research and growing popularity in various applications, we provide a comprehensive survey of VLMs. Specifically, we provide a systematic overview of VLMs in the following aspects: [1] model information of the major VLMs developed up to 2025; [2] the transition of VLM architectures and the newest VLM alignment methods; [3] summary and categorization of the popular benchmarks and evaluation metrics of VLMs; [4] the challenges and issues faced by current VLMs such as hallucination, alignment, and safety.

[Open PDF in Zotero](zotero://select/items/@liSurveyStateArt2025)
