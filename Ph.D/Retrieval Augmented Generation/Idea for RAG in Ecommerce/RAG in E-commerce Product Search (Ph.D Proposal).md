# Introduction

Though Large Language Models (LLMs) excel at natural language processing and generation, their knowledge is limited to the training corpus. They may fail or hallucinate when required to perform tasks needing knowledge beyond their internal knowledge. Retrieval-Augmented Generation (RAG) is a technique that addresses this limitation by retrieving knowledge from external sources and providing it as contextual information to the LLM. This allows the model to access external knowledge necessary to perform tasks more accurately and reliably.

In e-commerce, the product search feature is essential as it serves as the primary interface for users to access desired products. Ensuring that all relevant products are displayed in response to user queries is critical for customer satisfaction and for promoting sales across the catalog. However, most queries are not simple keyword-based requests such as “laptop” or “earphones” that directly map to product names or categories. Users often do not know exactly what they are searching for. For example, a query like “a powerful computer for an AI professional” requires understanding of unstructured product knowledge to identify relevant products. Here, the strength of LLMs lies in their ability to process and interpret unstructured natural language, while RAG provides the mechanism to integrate external product knowledge into LLMs, enabling the implementation of powerful semantic product search in e-commerce systems.

Therefore, this research will conduct experiments to determine the most effective way to integrate RAG mechanisms into e-commerce product search. In addition, it will address practical aspects such as minimizing latency and energy consumption to ensure that the system can be deployed in a cost-effective and profitable manner.

# Problem Statement


This research aims to develop a system that integrates Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) into a large-scale e-commerce search platform. The target environment involves more than 4 million products, 14 million offers, and approximately 2 million daily search queries across 600 stores. At this scale, search must deliver results with high accuracy while meeting strict operational requirements.

Integrating Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) offers the potential to improve search quality, but doing so in this setting raises critical challenges. These include ensuring response times below 300 milliseconds, minimizing energy consumption for in-house deployments, controlling costs when using external LLM services, and designing hybrid architectures that balance efficiency and accuracy. Addressing these constraints is essential to make such a system usable and profitable in real-world e-commerce.

# Objectives

To ensure the proposed system is practical and usable in a real-world e-commerce context, the research will pursue the following objectives:
1. Make sure answers generated by Large Language Models (LLMs) are relevant and grounded to the internal product and offer data. 
2. Incorporate external data sources (e.g., Amazon, Google, user-generated content) to enrich system knowledge and capture real-world usage.
3. Ensure 300 milliseconds or less response time for each user query.
4. Minimize energy consumption when deploying in-house LLM.
5. Assess whether external LLM services can be used cost-effectively at scale.
6. Implement a hybrid architecture combining traditional search for short queries and LLMs for longer ones for efficient computation. 
7. Conduct a critical assessment of commercial solutions (e.g. Google Vertex AI Agent Builder) to benchmark against the proposed solution. 
# State of the Art Analysis

RAG systems involve two main components: a retriever and a generator. When a user provide a complete / partial query, a retriever extract knowledge from the sources that is relevant to the input and provide it to the LLM to generate answer to the query [1]. Depending on those components, RAG researches have four directions in general: retriever oriented, generator oriented, hybrid and robustness oriented approaches.  

Retriever oriented RAG system focus on optimizing the retriever part of the system by refining input query , enhancing retriever architecture, filtering irrelevant information from the passive, and compressing long information into compact efficient representation [1]. 

Generator-oriented RAG systems emphasize improving the generation process. Common strategies include faithfulness-aware decoding, context compression, utility filtering, and generation control [1]. These methods refine the model’s output, ensure that the generated text remains grounded in retrieved passages, compress encoder outputs across multiple passages to fit within the LLM’s context window, adjust reranking strategies based on task type and retrieval metadata, and employ agentic decision-making so that only knowledge relevant to the task is preserved.

On the other hand, hybrid RAG systems involve both retriever and generator. Instead of treating them as individual components, they focus on the coordinating between the two components. The trends of developing hybrid RAG systems involves iterating between the retriever and generator to perform continuous evidence and answer refinement, applying reinforce-based gradients to adapt both components, implementing dynamic retrieval mechanism during the generation uncertainty [1]. 

Finally, robustness and security-oriented RAG systems focus on improving robustness against irrelevant or misleading context, detecting incorrect or hallucinating information in the generation, and preventing semantic backdoors attacks trigged by intentionally passages to prevent possible security compromise [1]. 

In the e-commerce domain, research on RAG remains limited, largely due to the scarcity of large-scale datasets. Product search datasets require detailed product information, user reviews, search logs, and user journey data—all of which raise privacy concerns and business restrictions that hinder open research [6]. Within this constrained landscape, three representative works are particularly relevant:

- **ProductRAG (Amazon Research)** [2] – a framework designed to improve query auto-completion (QAC) by generating precise suggestions grounded in product knowledge and aligned with user search prefixes.
- **BSharedRAG** [3] – a system that employs a single LLM backbone fine-tuned on an e-commerce corpus, with LoRA adapters applied for both retrieval and generation tasks. By sharing information between retriever and generator through the adapters, it demonstrates improved alignment and efficiency.
- **Graph-based RAG framework** [7] – a method that constructs an item–feature knowledge graph from user reviews and integrates it into retrieval. By leveraging graph traversal to explore explicit and implicit item–feature connections, this approach enhances the diversity, fidelity, and grounding of generated product descriptions.

Together, these studies represent the most relevant and reputable RAG approaches for e-commerce to date, but they also reveal gaps in efficiency, scalability, and evaluation that our work seeks to address.

Another important dimension is the energy consumption of LLMs during inference. Recent research has advanced rapidly in this area. Popular approaches include vLLM with PagedAttention to reduce memory fragmentation and improve GPU utilization, speculative decoding, and adaptive GPU frequency adjustment [12], [14]. Hardware-oriented solutions such as throttLL’eM dynamically modulate GPU speed in real time, aligning power consumption with current demand while still meeting latency objectives [13]. An effective combination of these methods has been shown to reduce the energy consumption of LLM inference by up to 73% compared to unoptimized baselines [12].
# Propose Approach

In this section, several methods are proposed to achieve the objectives of the research.
### RAG Architecture

To implement an effective and efficient RAG based product system for real-world e-commerce, the proposed architecture consists of three main processes:
- Query Classification and Data Chunking
- A retriever
- A generator (optional in some case)

**Query Classification and Data Chunking**

The first step is query classification to decide the most appropriate mechanism for fulfilling a given query. A hybrid strategy is proposed: combining traditional keyword-based search for simple queries with RAG-based search using LLMs for more complex ones. This hybrid mechanism reduces both cost and energy consumption while maintaining high-quality results.

In addition, an intermediate step (data chunking) is introduced to fetch a subset of product or offer data using SQL, Python, or other data modeling languages prior to the retrieval process. Given the scale of tens of millions of records, computing similarity scores across the entire dataset is infeasible. Instead, focusing on smaller, category-based chunks significantly improves efficiency. Furthermore, parallel processing of these chunks can accelerate the retrieval process.

Effective chunking requires accurate understanding of user intent and comprehensive knowledge of products and offers. To improve query understanding, methods from prior research can be adopted. RQ-RAG enhances user intent representation through rewriting, decomposition, and disambiguation [5]. RAG-Fusion expands a query into multiple sub-queries to cover alternative interpretations and subtopics [10]. GMR applies reasoning chains to model multi-hop queries step by step, improving retrieval accuracy [9].

**Retriever**

Since user intent is already refined in the previous step, the retriever focuses on efficiently matching queries to the most relevant products. Re2G (Retrieve, Rerank, Generate) [17] demonstrates the value of combining symbolic and neural retrieval with reranking layers to improve precision, while LongRAG [15] retrieves compressed long-context chunks through document grouping, enabling better use of long-context language models in large-scale settings. In addition, FILCO (Filter Context) [16] improves retrieval granularity by filtering out irrelevant or low-utility spans before generation, enhancing both faithfulness and efficiency. These approaches provide practical strategies to strengthen retrieval quality and scalability in e-commerce systems.

**Generator**

The generator is optional in product search scenarios where only ranked product lists are required. However, it becomes essential in chatbot-style systems that simulate a virtual salesperson. The generator produces natural language answers that are relevant to the query and grounded in retrieved product and offer data. To improve reliability, methods such as self-reflection, verification, correction, revisiting, and refining can be applied [4], [11].

### Dataset

- The main internal dataset consists of 4 million products and 14 million offers, serving as the primary knowledge base for retrieval.
- External knowledge sources such as Amazon, Google, and user reviews can be integrated to expand product knowledge and improve coverage [7].
- Vector representations will be created for all data to enable retrieval via similarity search.
- These representations will be stored separately from the transactional database to improve scalability and performance.
- Benchmark datasets can be constructed by combining historical query logs with synthetic queries and answers generated by LLMs [8].

### Evaluation

To ensure that the proposed system is practical and deployable in real-world e-commerce settings, several evaluation dimensions will be considered:
1. **Context Relevance, Answer Faithfulness and Answer Relevancy** – Evaluate how effectively the system retrieves products. LLM judges such as DeBERTa-v3-Large models can be used to perform binary classification [8].
2. **Efficiency (Latency)** – Evaluate average response times for the completion of each query.
3. **Energy Consumption** – Track GPU power usage during inference using the NVIDIA Management Library [12].
4. **Robustness** – Assess tolerance to noisy, ambiguous, or adversarial queries.
5. **Cost and Throughput** – Measure cost per 1,000 queries and tokens/query for paid LLMs, as well as overall query-per-second (QPS) throughput.
6. **Benchmarking Against Commercial Solutions** – Conduct a critical assessment of commercial systems such as Google Vertex AI Agent Builder. This will involve running the same queries on both the proposed system and commercial solutions, then comparing them across relevance, latency, energy consumption, robustness, and cost. The comparison will provide a baseline to evaluate the practical advantages and trade-offs of the proposed approach.
# Alignment of Objectives and Proposed Methods
### Make sure answers generated by Large Language Models (LLMs) are relevant and grounded to the internal product and offer data. 

This will be achieved by grounding generation in the internal dataset of 4 million products and 14 million offers, represented as vectors for efficient retrieval. Reliability will be further strengthened through generator-level methods such as self-reflection and verification [4], [11]. In addition, a synthetic benchmark constructed from historical query logs and LLM-generated queries will be used to evaluate the relevance and fidelity of the proposed system.

### Incorporate external data sources (e.g., Amazon, Google, user-generated content) to enrich system knowledge and capture real-world usage.

This objective will be addressed by integrating external sources such as Amazon, Google, and user-generated reviews to complement the internal catalog of 4 million products and 14 million offers. Following the approach of the graph-based RAG framework [7], user reviews and external product information can be transformed into an item–feature knowledge graph and vectorized for retrieval. By leveraging graph traversal to capture explicit and implicit item–feature connections, these external sources enrich the knowledge base, improving coverage, diversity, and grounding of product search results.

### Ensure 300 milliseconds or less response time for each user query.

The proposed query classification and data chunking mechanism reduces the search space, while parallel processing of category-based chunks ensures fast similarity matching, making sub-300 ms latency feasible. In addition, pre-computing vector embeddings for all products and offers enables rapid retrieval without repeated on-the-fly computation, further improving efficiency.
### Minimize energy consumption  when deploying in-house LLM.

Energy efficiency will be addressed by integrating inference optimizations such as vLLM with PagedAttention, speculative decoding, and adaptive GPU frequency adjustment [12], [14], applied during the generator phase.
### Assess whether external LLM services can be used cost-effectively at scale.

The proposed query classification step will determine when external LLM services are necessary, depending on the complexity and use case of the query. This selective routing reduces unnecessary reliance on external APIs. The evaluation framework will then measure cost per 1,000 queries and tokens/query for external services and compare them with optimized in-house deployments to assess cost-effectiveness in real workloads.
### Implement a hybrid architecture combining traditional search for short queries and LLMs for longer ones for efficient computation. 

The proposed query classification step routes simple keyword queries directly to traditional search, while complex or descriptive queries are processed by the RAG pipeline, optimizing both efficiency and quality.
### Conduct a critical assessment of commercial solutions (e.g. Google Vertex AI Agent Builder) to benchmark against the proposed solution. 

Benchmarking experiments will run identical query sets across the proposed RAG system and commercial platforms, comparing relevance, latency, energy consumption, robustness, and cost. This comparison will highlight whether the proposed system provides measurable improvements in efficiency, scalability, and result quality compared to commercial alternatives.
# References

[1] C. Sharma, “Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers,” May 28, 2025, arXiv: arXiv:2506.00054. doi: 10.48550/arXiv.2506.00054.
[2] F. Sun et al., “A Product-Aware Query Auto-Completion Framework for E-Commerce Search via Retrieval-Augmented Generation Method,” 2024.
[3] K. Guan, Q. Cao, Y. Sun, X. Wang, and R. Song, “BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain,” Sept. 30, 2024, arXiv: arXiv:2409.20075. doi: 10.48550/arXiv.2409.20075.
[4] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=hSyW5go0v8
[5] Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. 2024. RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation. In First Conference on Language Modeling. https://openreview.net/forum?id=tzE7VqsaJ4
[6] M. Tsagkias, T. H. King, S. Kallumadi, V. Murdock, and M. De Rijke, “Challenges and research opportunities in eCommerce search and recommendations,” SIGIR Forum, vol. 54, no. 1, pp. 1–23, June 2020, doi: 10.1145/3451964.3451966.
[7] J. Yang, Y. Jia, C. Yang, Y. Liang, and L. Lin, “Boosting E-commerce Content Diversity: A Graph-based RAG Approach with User Reviews,” in Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2, Toronto ON Canada: ACM, Aug. 2025, pp. 3495–3506. doi: 10.1145/3711896.3736864.
[8] J. Saad-Falcon, O. Khattab, C. Potts, and M. Zaharia, “ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems,” Mar. 31, 2024, arXiv: arXiv:2311.09476. doi: 10.48550/arXiv.2311.09476.
[9] Z. Rackauckas, “RAG-Fusion: a New Take on Retrieval-Augmented Generation,” IJNLC, vol. 13, no. 1, pp. 37–47, Feb. 2024, doi: 10.5121/ijnlc.2024.13103.
[10] H. Lee, S. Yang, H. Oh, and M. Seo, “Generative Multi-hop Retrieval,” Oct. 16, 2022, arXiv: arXiv:2204.13596. doi: 10.48550/arXiv.2204.13596.
[11] X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao, and R. Yan, “Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory”.
[12] J. Fernandez, C. Na, V. Tiwari, Y. Bisk, S. Luccioni, and E. Strubell, “Energy Considerations of Large Language Model Inference and Efficiency Optimizations,” 2025, arXiv. doi: 10.48550/ARXIV.2504.17674.
[13] A. K. Kakolyris, D. Masouros, P. Vavaroutsos, S. Xydis, and D. Soudris, “SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving,” Aug. 05, 2024, arXiv: arXiv:2408.05235. doi: 10.48550/arXiv.2408.05235.
[14] J. Stojkovic, E. Choukse, C. Zhang, I. Goiri, and J. Torrellas, “Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference,” Mar. 29, 2024, arXiv: arXiv:2403.20306. doi: 10.48550/arXiv.2403.20306.
[15] Z. Jiang, X. Ma, and W. Chen, “LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs,” Sept. 01, 2024, arXiv: arXiv:2406.15319. doi: 10.48550/arXiv.2406.15319.
[16] Z. Wang, J. Araki, Z. Jiang, M. R. Parvez, and G. Neubig, “Learning to Filter Context for Retrieval-Augmented Generation,” Nov. 14, 2023, arXiv: arXiv:2311.08377. doi: 10.48550/arXiv.2311.08377.
[17] M. Glass, G. Rossiello, M. F. M. Chowdhury, A. R. Naik, P. Cai, and A. Gliozzo, “Re2G: Retrieve, Rerank, Generate,” July 13, 2022, arXiv: arXiv:2207.06300. doi: 10.48550/arXiv.2207.06300.

