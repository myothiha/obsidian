
# Introduction

Though Large Language Models (LLMs) excel at natural language processing and generation, their knowledge is limited to the training corpus. They may fail or hallucinate when required to perform tasks needing knowledge beyond their internal knowledge. Retrieval-Augmented Generation (RAG) is a technique that addresses this limitation by retrieving knowledge from external sources and providing it as contextual information to the LLM. This allows the model to access external knowledge necessary to perform tasks more accurately and reliably.

In e-commerce, the product search feature is essential as it serves as the primary interface for users to access desired products. Ensuring that all relevant products are displayed in response to user queries is critical for customer satisfaction and for promoting sales across the catalog. However, most queries are not simple keyword-based requests such as “laptop” or “earphones” that directly map to product names or categories. Users often do not know exactly what they are searching for. For example, a query like “a powerful computer for an AI professional” requires understanding of unstructured product knowledge to identify relevant products. Here, the strength of LLMs lies in their ability to process and interpret unstructured natural language, while RAG provides the mechanism to integrate external product knowledge into LLMs, enabling the implementation of powerful semantic product search in e-commerce systems.

Therefore, this research will conduct experiments to determine the most effective way to integrate RAG mechanisms into e-commerce product search. In addition, it will address practical aspects such as minimizing latency and energy consumption to ensure that the system can be deployed in a cost-effective and profitable manner.

# Problem Statement


This research aims to develop a system that integrates Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) into a large-scale e-commerce search platform. The target environment involves more than 4 million products, 14 million offers, and approximately 2 million daily search queries across 600 stores. At this scale, search must deliver results with high accuracy while meeting strict operational requirements.

Integrating Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) offers the potential to improve search quality, but doing so in this setting raises critical challenges. These include ensuring response times below 300 milliseconds, minimizing energy consumption for in-house deployments, controlling costs when using external LLM services, and designing hybrid architectures that balance efficiency and accuracy. Addressing these constraints is essential to make such a system usable and profitable in real-world e-commerce.

# Objectives

To ensure the proposed system is practical and usable in a real-world e-commerce context, the research will pursue the following objectives:
1. Guarantee that answers generated by Large Language Models (LLMs) are relevant and grounded in internal product and offer data.
2. Achieve response times of 300 milliseconds or less for each user query.
3. Minimize energy consumption when deploying in-house LLMs.
4. Evaluate the cost-effectiveness of using external LLM services at scale.
5. Design and implement a hybrid architecture that combines traditional search for short queries with LLMs for longer ones to optimize efficiency.
6. Conduct a critical assessment of commercial solutions (e.g., Google Vertex AI Agent Builder) to establish benchmarks for comparison with the proposed system.
# State of the Art Analysis

RAG systems involve two main components: a retriever and a generator. When a user provide a complete / partial query, a retriever extract knowledge from the sources that is relevant to the input and provide it to the LLM to generate answer to the query [1]. Depending on those components, RAG researches have four directions in general: retriever oriented, generator oriented, hybrid and robustness oriented approaches.  

Retriever oriented RAG system focus on optimizing the retriever part of the system by refining input query , enhancing retriever architecture, filtering irrelevant information from the passive, and compressing long information into compact efficient representation [1]. 

Generator-oriented RAG systems emphasize improving the generation process. Common strategies include faithfulness-aware decoding, context compression, utility filtering, and generation control [1]. These methods refine the model’s output, ensure that the generated text remains grounded in retrieved passages, compress encoder outputs across multiple passages to fit within the LLM’s context window, adjust reranking strategies based on task type and retrieval metadata, and employ agentic decision-making so that only knowledge relevant to the task is preserved.

On the other hand, hybrid RAG systems involve both retriever and generator. Instead of treating them as individual components, they focus on the coordinating between the two components. The trends of developing hybrid RAG systems involves iterating between the retriever and generator to perform continuous evidence and answer refinement, applying reinforce-based gradients to adapt both components, implementing dynamic retrieval mechanism during the generation uncertainty [1]. 

Finally, robustness and security-oriented RAG systems focus on improving robustness against irrelevant or misleading context, detecting incorrect or hallucinating information in the generation, and preventing semantic backdoors attacks trigged by intentionally passages to prevent possible security compromise [1]. 

In the e-commerce domain, research on RAG remains limited, largely due to the scarcity of large-scale datasets. Product search datasets require detailed product information, user reviews, search logs, and user journey data—all of which raise privacy concerns and business restrictions that hinder open research [6]. Within this constrained landscape, three representative works are particularly relevant:

- **ProductRAG (Amazon Research)** [2] – a framework designed to improve query auto-completion (QAC) by generating precise suggestions grounded in product knowledge and aligned with user search prefixes.
- **BSharedRAG** [3] – a system that employs a single LLM backbone fine-tuned on an e-commerce corpus, with LoRA adapters applied for both retrieval and generation tasks. By sharing information between retriever and generator through the adapters, it demonstrates improved alignment and efficiency.
- **Graph-based RAG framework** [7] – a method that constructs an item–feature knowledge graph from user reviews and integrates it into retrieval. By leveraging graph traversal to explore explicit and implicit item–feature connections, this approach enhances the diversity, fidelity, and grounding of generated product descriptions.

Together, these studies represent the most relevant and reputable RAG approaches for e-commerce to date, but they also reveal gaps in efficiency, scalability, and evaluation that our work seeks to address.

Another important dimension is the energy consumption of LLMs during inference. Recent research has advanced rapidly in this area. Popular approaches include vLLM with PagedAttention to reduce memory fragmentation and improve GPU utilization, speculative decoding, and adaptive GPU frequency adjustment [12], [14]. Hardware-oriented solutions such as throttLL’eM dynamically modulate GPU speed in real time, aligning power consumption with current demand while still meeting latency objectives [13]. An effective combination of these methods has been shown to reduce the energy consumption of LLM inference by up to 73% compared to unoptimized baselines [12].
# Propose Approach

In this section, several methods are proposed to achieve the objectives of the research.
### RAG Architecture

To implement an effective and efficient RAG based product system for real-world e-commerce, the proposed architecture consists of three main processes:
- Query Classification and Data Chunking
- A retriever
- A generator (optional in some case)

**Query Classification and Data Chunking**

The first step is query classification to decide the most appropriate mechanism for fulfilling a given query. A hybrid strategy is proposed: combining traditional keyword-based search for simple queries with RAG-based search using LLMs for more complex ones. This hybrid mechanism reduces both cost and energy consumption while maintaining high-quality results.

In addition, an intermediate step (data chunking) is introduced to fetch a subset of product or offer data using SQL, Python, or other data modeling languages prior to the retrieval process. Given the scale of tens of millions of records, computing similarity scores across the entire dataset is infeasible. Instead, focusing on smaller, category-based chunks significantly improves efficiency. Furthermore, parallel processing of these chunks can accelerate the retrieval process.

Effective chunking requires accurate understanding of user intent and comprehensive knowledge of products and offers. To improve query understanding, methods from prior research can be adopted. RQ-RAG enhances user intent representation through rewriting, decomposition, and disambiguation [5]. RAG-Fusion expands a query into multiple sub-queries to cover alternative interpretations and subtopics [10]. GMR applies reasoning chains to model multi-hop queries step by step, improving retrieval accuracy [9].

Retriever

Generator

The generator is optional in product search scenarios where only ranked product lists are required. However, it becomes essential in chatbot-style systems that simulate a virtual salesperson. The generator produces natural language answers that are relevant to the query and grounded in retrieved product and offer data. To improve reliability, methods such as self-reflection, verification, correction, revisiting, and refining can be applied [4], [11].
(original)

**The Retriever**

 After that, the retriever need to match user's input queries to relevant products from the chunks. For this task, understanding user intent and comprehensive products and offers knowledge are essential. To improve the understanding of user intent, we can adopt method from some researches such as RQ-RAG [5], GMR [9] and RAG-Fusion [10]. RQ-RAG improve user intent by rewriting, decomposition and disambiguation to have a clearer query before the retrieval process [5]. On the other hand, RAG-fusion breakdown the original query into multiple sub and alternative queries to cover different interpretations and subtopics in several contents [10]. Similarly, GMR also focus on user queries with multiple hops, but it use reasoning chains to generate step by step modeling to retrieve the relevant information [9]. 

**The Generator**

A generator is optional for the product search system where only natural language generation is not required. However, it is essential for the chatbot based product search system which can act as a sales person. The main task of a generator is to output an answer that is relevant to the query and grounded in the retrieved products and offer data. To ensure that we can adopt proven methods from current research using self-reflection, verification, correction, revisiting and refining  [4], [11]. 

### Dataset
- Current E-commerce have 2 million product and 14 million offer data which will be the main external datasource to be retrieved by RAG system.
- In addition, we can integrate the external knowledge about the product such as amazon, google, and user reviews to expand the products' knowledge base [7]. 
- Then, we can create vector representation of all data to be ready for the retrieval process. 
- We can store those information separate from the main data source. 
- Then, create a benchmark if we have user previous query logs and expand it with synthetic queries and answers for each product and offer using LLM [8].

### Evaluation

We need to evaluate perform several evaluation on different components and different aspect of the proposed to make sure it usable in the real world E-commerce setting. There are several evaluation dimension for this system.
1. Context Relevance: evaluate how the proposed system fetch products and offers relevant to the user queries. We can use nDCG@10, MRR, Success@1, Recall@k for the measurement of relevancy.
2. Efficiency (Latency): evaluate average response times.
3. Energy consumption for the queries - GPU power metrics can be measured using Nvidia Management Library [12]. 
4. Robustness: check the noise tolerance of the proposed system.
5. Cost & Throughput: Euro per 1k queries, tokens/query for paid LLMs. 

### Make sure answers generated by Large Language Models (LLMs) are relevant and grounded to the internal product and offer data. 

For a product search feature, the generation component of a RAG system is not always necessary, since the retriever can directly fetch relevant products according to the user query and return them as they are. The generator becomes essential mainly when e-commerce platforms deploy LLM-powered sales support systems, such as conversational chatbots. Nevertheless, rigorous evaluation of both the retriever and the generator is critical for selecting the most effective approaches and models. The evaluation system must measure whether the retrieved products and offers, as well as the responses generated by LLMs, remain relevant and faithfully grounded in the internal product and offer information.

ARES proposes an evaluation framework that builds synthetic queries and responses by leveraging existing external product and offer data [8]. LLM judges are then trained to classify RAG outputs along three dimensions: relevance, faithfulness, and answer relevance score. In addition, ARES introduces a PPI rectifier function derived from a small set of human preferences  [8]. This rectifier is used to estimate the error rates of the LLM judges and to generate confidence intervals for both the success and failure rates of the RAG system.

### Ensure 300 milli seconds or less response time for each user query.

There are many aspect to work on to improve response of the user queries. 
##### Optimizing Retrieval Process

Since there are 4 million products and 14 million offers data, the retrieval process will be the bottleneck since we have to match user queries with the products. Most e-commerce systems use one or more database management system such as MySQL, Oracle, MongoDB or Neo4J to store their products and offers data. We can use LLM to generate a query language for the target DBMS to select the most appropriate subset of the data first before the retrieval process begin. For example, if the user queries is related to mobile phones, a subset of projects with "phone" or even broader "electronic" categories should be fetched and perform the retrieval process on the subset which till cut the processing time significantly.  In addition, we can compute embedding for all products to accommodate faster matching process during retrieval. [2]. 

We can also arrange dynamic retrieval mechanisms for different complexity of queries: traditional keyword search for simple queries, retrieval only for medium complexity queries and LLM based retrieval and generation for highly complex queries. We can experiments with many different retriever such as BERT, different size and variations of LLM and balance the accuracy and efficiency and choose the most balance one. In addition, we can deploy multiple small efficient models to handle multiple users requests at the same time. 

### Minimize energy consumption  when deploying in-house LLM.

### Assess whether external LLM services can be used cost-effectively at scale.

### Implement a hybrid architecture combining traditional search for short queries and LLMs for longer ones for efficient computation. 

A model to classify each user queries into the following categories.
- Queries that can be deal with traditional search.
- Queries that require reasoning ability of small LLMs.
- Queries that require to use external LLM service.
### Conduct a critical assessment of commercial solutions (e.g. Google Vertex AI Agent Builder) to benchmark against the proposed solution. 


[1] C. Sharma, “Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers,” May 28, 2025, arXiv: arXiv:2506.00054. doi: 10.48550/arXiv.2506.00054.
[2] F. Sun et al., “A Product-Aware Query Auto-Completion Framework for E-Commerce Search via Retrieval-Augmented Generation Method,” 2024.
[3] K. Guan, Q. Cao, Y. Sun, X. Wang, and R. Song, “BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain,” Sept. 30, 2024, arXiv: arXiv:2409.20075. doi: 10.48550/arXiv.2409.20075.
[4] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=hSyW5go0v8
[5] Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. 2024. RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation. In First Conference on Language Modeling. https://openreview.net/forum?id=tzE7VqsaJ4
[6] M. Tsagkias, T. H. King, S. Kallumadi, V. Murdock, and M. De Rijke, “Challenges and research opportunities in eCommerce search and recommendations,” SIGIR Forum, vol. 54, no. 1, pp. 1–23, June 2020, doi: 10.1145/3451964.3451966.
[7] J. Yang, Y. Jia, C. Yang, Y. Liang, and L. Lin, “Boosting E-commerce Content Diversity: A Graph-based RAG Approach with User Reviews,” in Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2, Toronto ON Canada: ACM, Aug. 2025, pp. 3495–3506. doi: 10.1145/3711896.3736864.
[8] J. Saad-Falcon, O. Khattab, C. Potts, and M. Zaharia, “ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems,” Mar. 31, 2024, arXiv: arXiv:2311.09476. doi: 10.48550/arXiv.2311.09476.
[9] Z. Rackauckas, “RAG-Fusion: a New Take on Retrieval-Augmented Generation,” IJNLC, vol. 13, no. 1, pp. 37–47, Feb. 2024, doi: 10.5121/ijnlc.2024.13103.
[10] H. Lee, S. Yang, H. Oh, and M. Seo, “Generative Multi-hop Retrieval,” Oct. 16, 2022, arXiv: arXiv:2204.13596. doi: 10.48550/arXiv.2204.13596.
[11] X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao, and R. Yan, “Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory”.
[12] J. Fernandez, C. Na, V. Tiwari, Y. Bisk, S. Luccioni, and E. Strubell, “Energy Considerations of Large Language Model Inference and Efficiency Optimizations,” 2025, arXiv. doi: 10.48550/ARXIV.2504.17674.
[13] A. K. Kakolyris, D. Masouros, P. Vavaroutsos, S. Xydis, and D. Soudris, “SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving,” Aug. 05, 2024, arXiv: arXiv:2408.05235. doi: 10.48550/arXiv.2408.05235.
[14] J. Stojkovic, E. Choukse, C. Zhang, I. Goiri, and J. Torrellas, “Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference,” Mar. 29, 2024, arXiv: arXiv:2403.20306. doi: 10.48550/arXiv.2403.20306.











